
# 1. 最大熵原理
**最大熵原理**：
保留全部不确定性，将熵达到最大，将风险降到最小。它指出，对一个随机事件的概率分布进行预测时，我们的预测应当满足全部已知的条件，而对未知的情况不要做任何主观假设。在这种情况下，概率分布最均匀，预测的风险最小。也就是我们常说的**不要把鸡蛋放到一个篮子里**。

匈牙利数学家希萨证明：**对任何一组不自相矛盾的信息，最大熵模型不仅存在，而且唯一**。

最大熵模型可以将多种信息整合到一个模型里。

# 2. 一个例子
拼音`wang xiao bo`既可以被转化为`王小波`，也可以被转化为`王晓波`；前一个`王小波`是作家，后一个`王晓波`是台湾学者。我们给出根据上下文（前两个词）和主题来预测下一个词（`王晓波`或者`王小波`）的最大熵模型：

![Imgur](http://i.imgur.com/02Ruk1j.png)

其中，`w3`是要预测的词，`w1,w2`是它的前两个词，`s`表示主题，`Z`是归一化因子，保证概率之和为1。

参数`λ`和`Z`需要通过观测数据训练得出。

# 3. 模型训练
最大熵模型在形式上很简单，但在实现上却很复杂，计算量非常大。

我们考虑20种特征，`{x1,x2,...,x20}`,要预测的变量为`d`。其最大熵模型为：

![Imgur](http://i.imgur.com/jLPDpRx.png)

其中

![Imgur](http://i.imgur.com/xeDSROY.png)

最原始的最大熵模型的训练使用**通用迭代算法**`GIS`(Generalized Iterative Scaling),它被概括为：

1. 假定第0次迭代的初始模型为等概率的均匀分布
2. 用第N次迭代的模型来估算每种信息特征在训练数据的分布。如果超过了实际的，就把相应的模型参数变小，否则把它们变大。
3. 重复步骤2，直至收敛。

后来，改进迭代算法IIS（Improved Iterative Scaling）被提出，使得训练时间缩短了一到两个数量级。
