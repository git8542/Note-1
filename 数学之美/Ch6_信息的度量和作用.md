**目录**

1. 信息熵
2. 信息的作用
	- 信息的作用
	- 条件熵
3. 互信息
4. 相对熵
	- 相对熵概念及性质
	- 詹森、香农度量

# 1. 信息熵
信息量和不确定性密切相关，不确定性越大，我们要了解它所需要的信息量越大。信息量（不确定性）用**信息熵**来度量。

**假设有一个变量`X`，它可能的取值有n多种，分别是`x1`，`x2`，……，`xn`， 每一种取到的概率分别是`P1`，`P2`，……，`Pn`，那么`X`的熵就定义为**：

![Imgur](http://i.imgur.com/1uoItRS.png)

### 信息冗余
**信息冗余度**是传输消息所用数据位的数目与消息中所包含的实际信息的数据位的数目的差值。比如有一个字符串`aaaaa`，在信源中，它所需要的存储空间为5个字节，也即它的真实信息量；但在信道中，我们可以使用两个字节的的`a5`来表示连续5个`a`，这个歌差就是冗余度。冗余度理论在数据压缩、通信中应用广泛。

# 2. 信息的作用
## （1）信息的作用
**信息是用来消除不确定性的**。

一件事情发生具有不确定性，假定不确定性为`U`,从外部消除这个不确定性唯一的办法是引入信息`I`,而需要引入的信息量取决于`U`的大小，即`I >= U`。

所以，为了减少一件事发生的不确定性，我们常常需要发掘一些隐含信息。

## （2）条件熵
定义在`Y`条件下，`X`的条件熵为：

![Imgur](http://i.imgur.com/d60O63I.png)

可以证明：`H(X) >= H(X|Y)`。

同理，两个条件下的条件熵定义为：

![Imgur](http://i.imgur.com/ljJ2BUV.png)

# 3. 互信息
**互信息用来度量两个随机变量`X`,`Y`之间的相关性**。它等于熵与条件熵的差，即：

![Imgur](http://i.imgur.com/aRImhT1.png)

当`X`与`Y`完全相关时，互信息为`H(X)`；当`X`与`Y`完全无关时，互信息为`0`。

# 4. 相对熵
## （1）相对熵概念及性质
**相对熵**（Relative Entropy）也称**KL散度**（Kullback-Leibler Divergence）、**交叉熵**, 它可以用来度量两个取值为正的函数的相似性（互信息是度量两个离散随机变量的相似性），定义为：

![Imgur](http://i.imgur.com/KdB4hJD.png)

它有三个性质：

1. 对于两个完全相同的函数，相对熵为`0`。
2. 相对熵越大，两个函数差异越大；反之，相对熵越小，两个函数差异越小。
3. 对于两个概率分布，相对熵可以度量两个概率分布的差异性。

## （2）詹森、香农度量
相对熵不对称，即：

![Imgur](http://i.imgur.com/GmVO4ET.png)

为了让它对称，詹森-香农提出了一种新的相对熵的计算方法，即

![Imgur](http://i.imgur.com/CHou0zw.png)

詹森-香农度量被用在Google的自动问答系统中，用来衡量两段信息的相似程度，来判断是否为抄袭。