**目录**

1. 表述
2. 简单数学描述
	- 定义
	- N-gram 模型
	- 条件概率估计
3. 实际处理
    - 高阶语言模型
    - 模型的训练、零概率问题和平滑方法

# 1.表述
**统计语言模型**是所有自然语言处理的基础，广泛应用于机器翻译、语音识别、印刷体或手写体识别、拼写纠错、汉字输入和文献查询。

# 2.简单数学描述
统计语言模型产生的初衷是为了解决**语音识别**问题。在语音识别中，计算机需要知道 **一个文字序列是否能构成一个大家理解而且有意义的句子** 。统计语言模型的出发点很简单：

	一个句子是否合理，就看它的概率大小如何

## （1）定义
假定`S`表示一个句子，由词`W1,W2,...,Wn`组成，`n`是句子长度。
句子概率`P(S)`可以表示为

	P(s) = P(W1,W2,...,Wn)

由条件概率公式得：

![条件概率](http://i.imgur.com/sGeMTmr.png)

词`Wn`出现的概率取决于它前面的所有词。从计算上来看，`P(W1)`比较容易计算,`P(W2|W1)`还不太麻烦,`P(W3|W1,W2)`就开始变得难算了，因为它涉及到了三个变量，每个变量的可能取值都是所有词。到最后一个条件概率我们就根本无法计算了。

## （2）N-gram 模型
为此，**马尔可夫假设**被提出:

	假设任意一个词Wi出现的频率只同它前面的词Wi-1有关。

于是`P(S)`可以写为:

![Imgur](http://i.imgur.com/eVSVCKh.png)

上式对应的统计语言模型是**Bigram Model**。我们可以假设一个词由前面`N-1`个词决定，对应的模型被称为**N-gram Model**。

## （3）条件概率估计
如何估计条件概率：`P(Wi|Wi-1)`，根据定义：

![条件概率](http://i.imgur.com/zgYCdG9.png)

我们只用估计联合概率（分子）和边缘概率（分母）即可。根据大数定理，只要统计量足够，我们可以用频率代替概率。所以我们只需统计出单词`Wi-1`和二元组`Wi-1，Wi`的频数即可。

# 3.实际处理

## （1）高阶语言模型
假定文本中的每个词`Wi`和前面`N-1`个词有关，而与更前面的词无关,这样当前词`Wi`的条件概率可以表示为：

![hh](http://i.imgur.com/3gDhuqT.png)

这种假设被称为**N-1阶马尔可夫假设**，对应的语言模型被称为**N-Gram Model**。`N`为`2`时为上节介绍的**Bigram Model**，`N`为`1`时是一个上下文无关模型。实际中应用最多的是`N=3`的三元模型。


**为何N的取值一般很小？**

1. N元模型的空间复杂度是N的指数函数，即`O(|V|^N)`,其中`|V|`为一种语言词典的词汇量。它的时间复杂度也是指数级，即`O(|V|^(N-1))`。
2. 当N从1到2，从2到3，模型的效果上升显著。但当N从3到4时，效果的提升就不是很显著，但资源的消耗却增加得非常快。



**三元、四元甚至更高阶的模型能够覆盖所有语言现象？**

答案当然不是。在自然语言中，上下文的相关性可能跨域非常大，甚至可能跨段落。这种情况下再提高模型也无济于事，这是马尔可夫假设的局限性，此时应采取其他一些**长程的依赖性**（Long Distance Dependency）来解决这个问题。

## （2）模型的训练、零概率问题和平滑方法

**模型的训练**

语言模型的参数就是模型中所有的条件概率。通过对语料的统计，得到这些参数的过程称为模型的训练。

**零概率问题**

求解二元模型`P(S)`，我们必须知道二元组`(Wi-1，Wi)`和词`Wi-1`的频数。但是当元组`(Wi-1， Wi)`的频数为0时，是否意味着条件概率`P(Wi|Wi-1)=0`？如果二元组`(Wi-1, Wi)`和`Wi-1`频数都为1，能否说明`P(Wi|Wi-1)=1`？显然并不是，因为此时观测值不足，**频率等于概率**并不被**大数定理**支持。

可以使用**增加数据**的方法，然而零概率依然会出现。如果模型的大部分条件概率为零，我们称这种现象为**不平滑**。
为了解决零概率问题，古德在图灵指导下，提出了一种**相信可靠的统计数据，对不可信统计数据打折扣**的一种概率估计方法。它将折扣出来的一小部分概率给予未看见的事件。

**古德--图灵估计**(Good–Turing frequency estimation)

描述：对于没有看见的事件，我们不能认为它发生的概率是零，因此我们需要从概率的总量（Probability Mass）中，分配一个很小的比例给这些没有看见的事件。（如下图）

![古德--图灵估计](http://images.cnitblog.com/blog/408927/201412/202122413296010.png)

以统计词典中每个词的频率为例来说明古德--图灵估计公式。
假定在语料库中出现`r`次的词有`Nr`个，特别地，未出现的词数量为![Imgur](http://i.imgur.com/3rrkMv2.png)。语料库的大小为`N`，那么很显然：

![Imgur](http://i.imgur.com/APkOIM6.png)  `(1)`

出现`r`次的词在整个语料库上的相对频度（Relative Frequency）则是`r/N`，如果不做任何处理，就以这个相对频度作为这些词的概率估计。

现假定当`r`比较小时，它的统计可能不可靠（不能根据大数定律），因此在计算这些词的概率时，要使用一个更小一点的频数`dr`，而不直接使用`r`。

古德--图灵估计按照下面的公式计算`dr`:

![Imgur](http://i.imgur.com/RsI1gMM.png)   `(2)`

显然：

![Imgur](http://i.imgur.com/wFcOQd0.png)   `(3)`

（将（2）式代入（3）式左部，再根据（1）式，即可得到（3）式右部）

一般来说，出现一次的词的数量比出现两次的多，出现两次的比三次的多。这种规律称为**Zipf定律**（Zipf’s Law）。下图是一个小语料上出现`r`次的词的数量`Nr`和`r`的关系。

![](http://images.cnitblog.com/blog/408927/201412/202211207981424.png)

当`r=0`时，根据公式（2）,`d0>0`。这样就给为出现的词赋予了一个很小的非零值，从而解决了零概率的问题。同时下调了出现概率很低的词的概率。

在实际的自然语言处理中，一般会设置一个阈值T，仅对出现次数小于T的词做上述调整。这样，对于频率超过一定阈值的词，它们的概率估计就是它们在语料库中的相对频度；对于频率小于这个阈值的词，它们的频率就小于它们的相对频度，出现次数越少折扣越多；对于未看见的词，也给予了一个比较小的频率。这样，所有词的概率估计就很平滑了。

**在二元模型运用平滑**

![Imgur](http://i.imgur.com/7DNaZgM.jpg)

这种平滑的方法，最早由前IBM 科学家卡茨（S. M. Katz) 提出，故称为卡茨退避法（Katz backoff）。类似地，对于三元模型，概率估计的公式如下：

![Imgur](http://i.imgur.com/sC2TNej.jpg)

<br />

**线性插值法**

用低阶语言模型和高阶模型进行线性插值来达到平滑的目的， 也是过去行业使用的一种方法， 这种方法称为删除差值（Deleted Interpolation），如下面的公式。该公式中的三个 λ 均为正数而且和为1。线性插值的效果略低于卡茨退避法（Backoff），故现在已经较少使用了。

![Imgur](http://i.imgur.com/2hZkOkj.jpg)





