目录

1. 基本原理
2. 实践
	- (1) gensim下
	- (2) tensorflow下

# 1. 基本原理
### 简介
Word2Vec是一种计算非常高效的，可以从原始语料中学习字词空间向量的预测模型。Word2Vec将每个词映射成一个向量，向量之间的距离可以用来度量每个词语义之间的相近程度。

### 原理及分类
主要分为CBOW（Continuous Bag of Words）和Skip-Gram两种模式，其中CBOW是从语境推测目标字词；而Skip-Gram是从目标字词推测语境。

### 适用范围
CBOW对小型数据比较合适，而Skip-Gram在大型语料中表现更好。

# 2. 实践
## （1）gensim下
gensim提供了非常顶层的api，输入是分过词的句子列表。
```python
from gensim.models import word2vec

texts = [
'I am a good student'.split(),
'Good good study day day up'.split()
]
model = word2vec.Word2Vec(texts, min_count=1)
# 打印单词'good'的词向量
print(model.wv.word_vec('good'))
# 打印和'good'相似的前2个单词
print(model.wv.most_similar('good', topn=2))
```

[More Info ——> Gensim models.word2vec](https://radimrehurek.com/gensim/models/word2vec.html)

## （2）tensorflow下
tensorflow下的api就相对底层些。

### 构造样本
我们主要训练Skip-Gram模式的Word2Vec，如前面所述，它是从目标字词推测语境。比如对于

	my name is brown

我们需要构造的样本就是

	name -> my
	name -> is
	is -> name
	is -> brown

### 核心代码

```python
graph = tf.Graph()
with graph.as_default():
    train_inputs = tf.placeholder(tf.int32, shape=[batch_size])
    train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])

    with tf.device('/cpu:0'):
        embeddings = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0))

        nce_weights = tf.Variable(
            tf.truncated_normal([vocab_size, embedding_size], stddev=1.0 / math.sqrt(embedding_size)))
        nce_biases = tf.Variable(tf.zeros([vocab_size]))

        input_embedding = tf.nn.embedding_lookup(embeddings, train_inputs)

    loss = tf.reduce_mean(
        tf.nn.nce_loss(weights=nce_weights, biases=nce_biases, labels=train_labels, inputs=input_embedding,
                       num_sampled=num_sampled, num_classes=vocab_size))
    optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)
    
    norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))
    normalized_embeddings = embeddings / norm

    init = tf.global_variables_initializer()

with tf.Session(graph=graph) as session:
    init.run()
    print('Initialized')
    for step in range(num_steps):
        batch_input, batch_labels = generate_batch(batch_size, num_skips, skip_window)
        feed_dict = {train_inputs: batch_input, train_labels: batch_labels}
        _, batch_loss = session.run([optimizer, loss], feed_dict=feed_dict)

    final_embeddings = normalized_embeddings.eval()
```

完整例子见 [tensorflow/tensorflow/examples/tutorials/word2vec/word2vec_basic.py](https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/tutorials/word2vec/word2vec_basic.py)

<br />

**Ref**

[Python: gensim: RuntimeError: you must first build vocabulary before training the model](http://stackoverflow.com/questions/33989826/python-gensim-runtimeerror-you-must-first-build-vocabulary-before-training-th)
